\documentclass[11pt,letterpaper]{article}

\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

\setlength{\textwidth}{6.5in}     
\setlength{\oddsidemargin}{0in}  
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{8.5in} 
\setlength{\topmargin}{0in}   
\setlength{\headheight}{14pt} 
\setlength{\headsep}{10pt}   
%\setlength{\footskip}{0in}

%------------------------------------------------
\newcommand{\homework}[2]{
\setcounter{section}{#1}
\section*{ICS635 Homework {\thesection}: {#2} }
{\markboth{#2}{#2}}
}
%------------------------------------------------


\begin{document}

% Enter the Homework number and title as arguments to
% homework
\homework{2}{by Lambert Leong}

\textbf{Problem 1}
\begin{enumerate}[labelindent=0pt]
%http://www.inference.org.uk/mackay/itprnn/ps/47.59.pdf
%http://allendowney.blogspot.com/2011/04/bayesianness-is-next-to-godliness.html
\item hypothesis - Oliver's blood is at the crime scene.\\
Probability Oliver was at the crime and the other person is AB blood type
	\begin{align*}
	P(O|D) &= \frac{P(D|O)P(O)}{P(D)}
	\end{align*}
Probability Oliver was not at the crime and that of 2 random people, 1 person is type O and the other is type AB
	\begin{align*}
	P(\tilde{O}|D) &= \frac{P(D|\tilde{O})P(\tilde{O})}{P(D)} 
	\end{align*}
Likelihood Ratio, Oliver was there/Oliver was not there
	\begin{align*}
		\frac{P(O|D)}{P(\tilde{O}|D)} &=
\frac{P(D|O)P(O)}{P(D|\tilde{O})P(\tilde{O})} \\
		\frac{P(O|D)}{P(\tilde{O}|D)} &= \frac{.01}{2*(.6)*(.01)} \\
		\frac{P(O|D)}{P(\tilde{O}|D)} &= .83\\
	\end{align*}
	The likelihood ratio suggest that it is more likely that the blood was
from two other people rather than the type O being from Oliver and the
other from a random person with type AB blood. 
\end{enumerate}

\noindent
\textbf{Problem 2}
\begin{enumerate}[labelindent=0pt]
% from Murphy 2012 page 76
\item 
	\begin{align*}
	\widehat{\theta}_{MLE} &= \frac{N_{1}}{N} \\
	&= \frac{1}{1}\\
	&=1
	\end{align*}

\item 
	\begin{align*}
		\widehat{\theta}_{MAP} &= \frac{N_{1}+a-1}{(N_{0}+N_{1})+a+b-2}\\
		&= \frac{1+1-1}{0+1+1+1-2}\\
		&= \frac{1}{1}
	\end{align*}
\item 
	\begin{align*}
		\bar{\theta} &= \frac{N_{1}+a}{N_{0}+N_{1}+a+b}\\
		&= \frac{1+1}{0+1+1+1}\\
		&=\frac{2}{3}
	\end{align*}
\item 
	\begin{align*}
	\iffalse
	p(\theta|x) &\propto p(x|\theta)p(\theta) \\
	& = \prod_{n}^{N}\theta^{x_{n}}(1-\theta)^{-x_{n}} \\
	&= \theta^{N_{1}}(1 - \theta) \\
	&= Beta(\theta| N_{1}+\alpha, N_{0}+\beta)\\
	&= \theta^{N_{1}+\alpha-1}(1-\theta)^{N_{0}+\beta-1}\\
	&=\\
		\\
	\fi
	p(\theta|x) &\propto p(x|\theta)p(\theta) \\
	&= \theta^{x}(1-\theta)^{1-x} \frac{\Gamma(N_{1}+\alpha + N_{0}+\beta))}{\Gamma(N_{1}+\alpha)\Gamma(N_{0}+\beta)} \theta^{N_{1}+\alpha - 1}(1-\theta)^{N_{0}+\beta-1} \\
	&= \frac{\Gamma(N_{1}+\alpha + N_{0}+\beta))}{\Gamma(N_{1}+\alpha)\Gamma(N_{0}+\beta)} \int_{0}^{1}\theta^{x + N_{1}+\alpha - 1}(1-\theta)^{N_{0}+\beta-x}d\theta \\
	&= \frac{\Gamma(N_{1}+\alpha + N_{0}+\beta))}{\Gamma(N_{1}+\alpha)\Gamma(N_{0}+\beta)} \frac{\Gamma(x+N_{1}+\alpha)\Gamma(N_{0}+\beta - x +1)}{\Gamma(N_{1}+\alpha+N_{0}+\beta+1)} \\
	&= \frac{\Gamma(1+1+ 0+1))}{\Gamma(1+1)\Gamma(0+1)} \frac{\Gamma(1+1+1)\Gamma(0+1- 0 +1)}{\Gamma(1+1+0+1+1)} \\
	&= \frac{\Gamma(3))}{\Gamma(2)\Gamma(1)} \frac{\Gamma(3)\Gamma(2)}{\Gamma(4)} \\
	\end{align*}
\iffalse
Or
	\begin{align*}
	p(\theta|x) \propto Beta(\theta|N_{1}+a,N_{0}+b)
	\end{align*}
\fi
\item 
\iffalse
	\begin{align*}
	p(x_{2} =1| x = 1, \theta) &= Beta(\theta| x_{2} = 1, x = 1) \\
	&= \theta^{x_{2}-1}(1-\theta)^{x -1}
	\end{align*}
\fi
	\begin{align*}
	p(x_{2} =1| x = 1, \theta) &=\frac{1}{Z(x+\alpha, N-x+\beta)}
\theta^{x+\alpha-1}(1-\theta)^{N-x+\beta-1}\\
	& = \frac{\theta^0(1-\theta)^0}{Z(1,1)}\\
	& = \frac{1}{Z(1,1)}\\
	& = \frac{\Gamma(1+1)}{\Gamma(1)\Gamma(1)}\\
	& = \frac{\Gamma(2)}{\Gamma(1)^2}
	\end{align*}
\item 
	\begin{align*}
	\widehat{\theta}_{MLE} &= \frac{N_{1}}{N} \\
	&= \frac{5}{5}\\
	&=1\\
	\\
	\widehat{\theta}_{MAP} &= \frac{N_{1}+a-1}{(N_{0}+N_{1})+a+b-2}\\
	&= \frac{5+1-1}{0+5+1+1-2}\\
	&= \frac{5}{5}\\
	&= 1 \\
	\\
	\bar{\theta} &= \frac{N_{1}+a}{N_{0}+N_{1}+a+b}\\
	&= \frac{5+1}{0+5+1+1}\\
	&=\frac{6}{7}\\
	\\
	p(\theta|x) &\propto p(x|\theta)p(\theta) \\
	&= \theta^{x}(1-\theta)^{1-x} \frac{\Gamma(N_{1}+\alpha + N_{0}+\beta))}{\Gamma(N_{1}+\alpha)\Gamma(N_{0}+\beta)} \theta^{N_{1}+\alpha - 1}(1-\theta)^{N_{0}+\beta-1} \\
	&= \frac{\Gamma(N_{1}+\alpha + N_{0}+\beta))}{\Gamma(N_{1}+\alpha)\Gamma(N_{0}+\beta)} \int_{0}^{1}\theta^{x + N_{1}+\alpha - 1}(1-\theta)^{N_{0}+\beta-x}d\theta \\
	&= \frac{\Gamma(N_{1}+\alpha + N_{0}+\beta))}{\Gamma(N_{1}+\alpha)\Gamma(N_{0}+\beta)} \frac{\Gamma(x+N_{1}+\alpha)\Gamma(N_{0}+\beta - x +1)}{\Gamma(N_{1}+\alpha+N_{0}+\beta+1)} \\
	&= \frac{\Gamma(5+1+ 0+1))}{\Gamma(5+1)\Gamma(0+1)} \frac{\Gamma(1+5+1)\Gamma(0+1- 0 +1)}{\Gamma(5+1+0+1+1)} \\
	&= \frac{\Gamma(7))}{\Gamma(6)\Gamma(1)} \frac{\Gamma(7)\Gamma(2)}{\Gamma(8)} \\
	\end{align*}
\item 
%https://github.com/ArthurZC23/Machine-Learning-A-Probabilistic-Perspective-Solutions/blob/master/3/15.pdf
	\begin{align*}
	m &= \frac{\alpha}{\alpha+\beta} \Rightarrow \beta = \alpha\frac{1-m}{m}\\
	v &= \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
	%Murphys 2012 page 78
	\end{align*}
	\begin{align*}
v &= \frac{\alpha(\frac{1-m}{m}))}{(\alpha+(\frac{1-m}{m})))^2(\alpha+(\alpha(\frac{1-m}{m}))+1)}\\
.\\
.\\
.\\
\alpha &= m\left(\frac{1-m}{v}-1\right)\\
\beta &= \alpha\frac{1-m}{m}=\left( m\left(\frac{1-m}{v}-1\right)\right)\left(\frac{1-m}{m}\right)
	\end{align*}
\end{enumerate}

\textbf{Problem 3}
%http://cross-entropy.net/ML310/homework03_answers.txt
\begin{enumerate}[labelindent=0pt]
\item 
	\begin{align*}
	\theta_{spam} = \frac{spam}{(spam + normal)} = \frac{3}{(3+4)} &=
\frac{3}{7} 
\\
	\theta_{secret|spam} = \frac{secret~ in~ spam}{(total~ spam)} =
\frac{2}{(2+1)} &= \frac{2}{3}
\\
	\theta_{secret|non-spam} = \frac{secret~ in~ non-spam}{(total~ non-spam)} =
\frac{1}{(2+2)} &= \frac{1}{4}
\\
	\theta_{sport|non-spam} = \frac{sport~ in~ non-spam}{(total~ non-spam)} =
\frac{2}{(2+2)} &= \frac{1}{2}
\\
	\theta_{dollar|spam} = \frac{dollar~ in~ spam}{(total~ spam)} =
\frac{1}{(1+2)} &= \frac{1}{3}
\\
	\end{align*}
\end{enumerate}

\textbf{Problem 4}
\begin{enumerate}[labelindent=0pt]
%http://scott.fortmann-roe.com/docs/BiasVariance.html
\item 
	The trade off between bias and variance is something to consider when
training complex models.  Increasing the data set may introduce more variance
which may introduce more error during training.  

\item
%http://emmanuel-klinger.net/leave-one-out-cross-validation-can-be-really-off.html
With equal classes of randomly labeled data, the best misclassification rate is
50\%.

Since the data is randomly labeled, when we leave one out, the classifier is more
likely to favor the class which has N data points rather than N-1.  It will then
misclassify the left out data point and leaving one out will misclassify on
every iteration.

\end{enumerate}
\textbf{Problem 5}
%\begin{enumerate}[labelindent=0pt]
%\end{enumerate}
\begin{table}[h!]
\begin{tabular}{|c|c|c|}
\hline
Model & Cross Validation & Accuracy \\ \hline
KNN & 0.7674 & 0.7865 \\ \hline
Gaussian Naive Bayes & 0.8563 & 0.8505 \\ \hline
LDA & 0.9073 & 0.892 \\ \hline
QDA & 0.9812 & 0.9815 \\ \hline
Decision Tree & 0.9872 & 0.9835 \\ \hline
XGBoost & 0.9439 & 0.9695 \\ \hline
\end{tabular}
\end{table}
\\
There are a lot of features and the first bit of preprocessing I did was feature
selection.  I used the SelectKBest function and varied k to be between a range
of 2 to 5 then trained the various models.  Feature selection did not seem to
yield better results which goes against my initial intuition.  High
dimensionality can be a problem for some of the models, for example KNNs.  I
performed cross validation for each model as an extra check against over fitting
and generalization error.  

The only hyper parameter that really seemed to make a difference in KNN is the
number of neighbors. I set my number of neighbors to be 5 however, I noticed,
that as the number of neighbors increases the accuracy improves for all odd
number neighbors.  I refrained from choosing an excessively high number of
neighbors to avoid possible over fitting.

There appears to be a significant class imbalance especially with regards to
class 1. When fitting the Gaussian naive Bayes classifier I used sample weights
to help mitigate the class imbalance.  The test set accuracy dropped slightly
when I used class weights but the cross validation accuracy stayed the same.

Using the least squares solver with auto shrinkage lead to good accuracies.  The
svd solver however, led to the best accuracies.  QDA resulted in high accuracies
for both testing and cross validation.  It is suspect because the accuracies are
near perfect and there may be possible over fitting.  Feature selection or
dimensionality reduction with something like PCA may help to reduce over fitting.
As mentioned before, feature selection did yield lower accuracies scores.

Decision tress are susceptible to problems with class imbalance.  I used sample
weights again during training and the cross validation accuracy did not increase
and the test accuracy decreased slightly.  I set a max depth of 3.  Accuracy
scores were high.  Lastly, I explored XGBoosting.  I set the number of
estimators to 50 and a max depth of 2.  The accuracies were slightly lower than
those from a single tree.   
\noindent\end{document}

\iffalse 

Consider a Naive Bayes model (multivariate Bernoulli version) for spam
classification with the vocabulary V="secret", "offer", "low", "price",
"valued", "customer", "today", "dollar", "million", "sports", "is", "for",
"play", "healthy", "pizza".  We have the example spam messages "million dollar
offer", "secret offer today", "secret is secret" and normal messages, "low
price for valued customer", "play secret sports today", "sports is healthy",
"low price pizza".  Give the MLEs for the following parameters:
theta_spam, theta_secret|spam, theta_secret|non-spam, theta_sports|non-spam,
theta_dollar|spam.

The multivariate Bernoulli version uses indicators: the term is present or not
present.

                                             customer    dollar    for    health
is    low    million    offer    pizza    play    price    secret    sports
today    valued
spam        million dollar offer                              1
1        1
            secret is secret
1                                                              1
            secret offer today
1                                   1                  1

                                             customer    dollar    for    health
is    low    million    offer    pizza    play    price    secret    sports
today    valued
non-spam    low price for valued customer           1                1
1                                             1
1
            low price pizza
1                            1                1
            play secret sports today
1                  1         1        1
            sports is healthy                                                  1
1                                                                        1

There are 3 spam messages and 4 non-spam messages.

theta_spam = 3 / (3 + 4) = 3 / 7
theta_secret|spam = 2 / (2 + 1) = 2 / 3
theta_secret|non-spam = 1 / (1 + 3) = 1 / 4
theta_sports|non-spam = 2 / (2 + 2) = 2 / 4 = 1 / 2
theta_dollar|spam = 1 / (1 + 2) = 1 / 3

\fi
